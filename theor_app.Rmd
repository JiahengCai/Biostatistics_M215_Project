---
title: "Theoretical Approach"
author: "Jiaheng Cai"
date: "11/9/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(KMsurv)
library(metaheuristicOpt)
library(survival)
```

This file is designated to explore the performance of metaheuristic algorithms on
likelihood estimation of survival analysis model. 

In real life, there are problems that we cannot give an analytical solution. 
Also, there are problems which we cannot use gradient-based optimization 
algorithms, since it is hard or impossible to calculate the derivative/gradient 
of the target function. This is the place where metaheuristic algorithms can take 
advantage. Also, metaheuristic algorithms tend to find global optimal points. 
We will use some examples to illustrate metaheuristic algorithms. We focus on the 
questions with analytical solution in order to discover the accuracy of metaheuristic
algorithms. 

# Introduction to some metaheuristics algorithms

## PSO

First, we will introduce PSO, particle swarm optimization. 
The idea of PSO is that, when a particle (e.g. a bird) is in a group (e.g. a flock)
of bird, the individual particle can profit from the information from all other 
individuals. The mathematical model is:

$$
\begin{align*}
X^i(t+1)&=X^i(t)+V^i(t+1)\\
V^i(t+1)&=wV^i(t)+c_1r_1(pbest^i-X^i(t))+c_2r_2(gbest-X^i(t))
\end{align*}
$$

The idea is that, we first generate P particles, there initial position is $X^i$.
Also, we assign a initial velocity to each particle, $V^i$. In each iteration, we 
update the velocity by the following criteria: 

- The velocity of last time, adjusted by a inertial parameter, $w$.
- The difference between particle i's personal best position, pbest 
(best minimize/maximize target function by this particle so far), and it's last
position. This difference is adjusted by a random number between 0 and 1, $r_1$,
and a cognitive parameter, $c_1$, which controls the effect of 'exploration'
- The difference between the global best position by all particle, gbest 
(best minimize/maximize target function by all particle so far), and particle i's 
last position. This difference is adjusted by a random number between 0 and 1, $r_2$,
and a social parameter, $c_2$, which controls the effect of 'exploitation'

## Cuckoo Search

The second algorithm is called Cuckoo Search, an algorithm mimic the behavior of
cuckoo bird. The idea is that, assume we have equal amount of nest (built by other 
birds), cuckoo bird and eggs. Each time the Cuckoo bird will lay 1 egg (exactly 1)
randomly in 1 nest. In nature, these eggs will face 2 scenario: abandoned or birth.
A parameter, $p_a$ is used to control for rate of abandoned eggs. After elimination 
process, the we keep the best egg (the values with best target value) and keep this
process. 

There are more metaheuristic algorithms. We'll use also use these algorithms with
shorter introduction. 

# Toy data

Let's start testing metaheuristic algorithms and compare it with analytical solution. 
We can first generate some data from known distributions which their likelihood.

## Normal Distribution

```{r}
xnorm = matrix(rnorm(100, 20, 13), 100, 1)
(mean(xnorm))
(var(xnorm))
```

```{r, results='hide'}
normallikeli = function(musigma2) {
  likeli = -(nrow(xnorm)/2)*log(2*pi) - (nrow(xnorm)/2)*log(musigma2[2]) - 
    (1/(2*musigma2[2]))*sum((xnorm - musigma2[1])^2)
  return(likeli)
} #normal distribution log likelihood

PSO(normallikeli, optimType = "MAX", numVar = 2, numPopulation = 100,
     maxIter = 1000, rangeVar = matrix(c(0,1000),2,2), Vmax = 1, ci = 1,
     cg = 1, w = 0.7)

SFL(normallikeli, optimType = "MAX", numVar = 5, numPopulation = 100,
maxIter = 1000, rangeVar = matrix(c(0,1000),2,2), numMemeplex = as.integer(100/3),
frogLeapingIteration = as.integer(10))

MFO(normallikeli, optimType = "MAX", numVar = 5, numPopulation = 100,
maxIter = 1000, rangeVar = matrix(c(0,1000),2,2))

HS(normallikeli, optimType = "MAX", numVar = 5, numPopulation = 100, maxIter = 1000,
rangeVar = matrix(c(0,1000),2,2), PAR = 0.3, HMCR = 0.95, bandwith = 0.05)
```

## Poisson Distribution

```{r}
xpoi = matrix(rpois(100, 7), 100, 1)
(mean(xpoi))
(var(xpoi))
```

```{r, results='hide'}
poilikeli = function(lambda) {
  likeli = sum(xpoi)*log(lambda) - nrow(xpoi)*lambda
  return(likeli)
} #normal distribution log likelihood

PSO(poilikeli, optimType = "MAX", numVar = 1, numPopulation = 100,
     maxIter = 1000, rangeVar = matrix(c(0,10),2,1), Vmax = 2, ci = 1.49445,
     cg = 1.49445, w = 0.729)

SFL(poilikeli, optimType = "MAX", numVar = 1, numPopulation = 100,
maxIter = 1000, rangeVar = matrix(c(0,10),2,1), numMemeplex = as.integer(100/3),
frogLeapingIteration = as.integer(10))
```

# Likelihood function for right-cencored data

This is a relatively easy question to discover. Recall that the likelihood function
for right-censored data:

$$
\begin{align*}
L&\propto\prod^{n}_{i=1}f(t_i)^{\delta_i}S(t_i)^{1-\delta_i}
\end{align*}
$$

A quick example, the 6-MP dataset:

```{r}
data(drug6mp)
```

If we consider the time to relapse of 6-MP group has a exponential distribution
with parameter $\lambda$, we can construct the following likelihood function:

$$
\begin{align*}
L&\propto\prod^{n}_{i=1}f(t_i)^{\delta_i}S(t_i)^{1-\delta_i}\\
&\propto(\lambda e^{-\lambda t_i})^{\delta_i}(e^{-\lambda t_i})^{1-\delta_i}\\
&\propto\lambda^{\sum^{n}_{i=1}\delta_i}e^{-\lambda\sum_{i=1}^nt_i}
\end{align*}
$$

The Log-likelihood is 

$$
\begin{align*}
l&\propto ln\lambda\sum_{i=1}^n\delta_i-\lambda\sum_{i=1}^nt_i
\end{align*}
$$

Take derivative against $\lambda$ and solve at 0

$$
\begin{align*}
\frac{\sum_{i=1}^n\delta_i}{\lambda}-\sum_{i=1}^nt_i &= 0\\
\hat{\lambda}&=\frac{\sum_{i=1}^n\delta_i}{\sum_{i=1}^nt_i}
\end{align*}
$$

Check for second derivative to confirm it is a maximum (skipped). If we write code in r, it will be like

```{r}
(lambdahat = sum(drug6mp$relapse) / sum(drug6mp$t2))
```

```{r}
explikeli <- function(lambda) {
  likeli = log(lambda)*sum(drug6mp$relapse) - lambda*sum(drug6mp$t2)
  return(likeli)
} #define the target function
```

This is a easy question, we can tackle it with analytical solution. Let's try
some metaheuristic algorithm. 

The following r code illustrate how PSO works (try some different number of 
iterations):

```{r,results='hide'}
iter = matrix(c(5,10,15,20,25,30,35,40,45,50), 1, 10)
PSOresult = matrix(0,1,10)
for (i in iter){
  PSOresult[i/5] = PSO(explikeli, optimType = "MAX", numVar = 1, numPopulation = 20, 
     maxIter = i, rangeVar = matrix(c(0,1),2,1), Vmax = 2, ci = 1.49445, 
     cg = 1.49445, w = 0.729)
}
```

```{r}
print(PSOresult)
```

As shown, the result from PSO converges as the iteration increase, and 

# Likelihood function for left-truncated, right-cencored data

Recall that when we do not have information of truncation time distribution, the 
likelihood function reduced to 

$$
\begin{align*}
L_c(S)&=\prod^{n}_{i}\frac{f(x_i^*)}{S(y_i^*)}
\end{align*}
$$

Recall that in the KM book, exercise 3.6, we constructed a likelihood function for
truncated data, ignore the information of truncation distribution:

$$\begin{align*}
L&=\prod_{i=1}^n\frac{f(x_i^*)^{\delta_i}S(x_i^*)^{1-\delta_i}}{S(y_i^*)}\\
&=\prod_{i=1}^n\frac{(\alpha\theta x_i^{\alpha -1})^{\delta_i}(e^{-\theta x_i^\alpha})^{\delta_i}(e^{-\theta x_i^\alpha})^{1-\delta_i}}{e^{-\theta y_i ^\alpha}}\\
&=\frac{(\alpha \theta)^4 89100^{\alpha -1}e^{-\theta (11^\alpha+12^\alpha+15^\alpha+33^\alpha+45^\alpha+28^\alpha)}}{e^{-\theta(5^\alpha+8^\alpha+12^\alpha+24^\alpha+32^\alpha+17^\alpha)}}
\end{align*}$$

This problem will be hard to give a analytical solution. Let's try metaheuristic 
algorithms

```{r}
ltrclikeli = function(alphatheta){
  exp1 = 11^alphatheta[1] + 12^alphatheta[1] + 15^alphatheta[1] + 33^alphatheta[1] 
  + 45^alphatheta[1] + 28^alphatheta[1]
  
  exp2 = 5^alphatheta[1] + 8^alphatheta[1] + 12^alphatheta[1] + 24^alphatheta[1] 
  + 32^alphatheta[1] + 17^alphatheta[1]
  
  likeli = ((alphatheta[1]*alphatheta[2])^4 * 89100^(alphatheta[1]-1) * exp(-alphatheta[2]*exp1))/(exp(-alphatheta[2]*exp2))
  return(likeli)
}

# alphatheta = matrix(c(1,1),2,1)
# ltrclikeli(alphatheta)
matrix(c(0,1,0,100),2,2)


# PSO(ltrclikeli, optimType = "MAX", numVar = 2, numPopulation = 100,
#      maxIter = 1000, rangeVar = matrix(c(0,1,0,10),2,2)
# , Vmax = 2, ci = 1.49445,
#      cg = 1.49445, w = 0.729)

# 
# SFL(ltrclikeli, optimType = "MAX", numVar = 5, numPopulation = 100,
# maxIter = 1000, rangeVar = matrix(c(0,10),2,2)
# , numMemeplex = as.integer(100/3),
# frogLeapingIteration = as.integer(10)) problematic

MFO(ltrclikeli, optimType = "MAX", numVar = 5, numPopulation = 100,
maxIter = 1000, rangeVar = matrix(c(0,100,0,1),2,2)
)

HS(ltrclikeli, optimType = "MAX", numVar = 5, numPopulation = 100, maxIter = 1000,
rangeVar = matrix(c(0,10),2,2)
, PAR = 0.3, HMCR = 0.95, bandwith = 0.05)
```

# Cox's Model

```{r}
data(hodg)
fit <- coxph(Surv(time, delta) ~ score, data = hodg)
summary(fit)
```

recall partial likelihood in our case:

```{r}
hodgt = hodg[order(hodg$time),]
row.names(hodgt) = NULL
matrix = matrix(0,nrow(hodg),4)

coxlikeli = function(beta){
  for (i in 1:43){
      matrix[i,1] = exp(beta*hodgt$score[i])
      
  }
  matrix[,2] = sum(matrix[,1]) - cumsum(matrix[,1]) + matrix[1,1]
  matrix[,3] = hodgt$delta
  matrix[,4] = (matrix[,1]/ matrix[,2])^matrix[,3]
  prod = prod(matrix[,4])
  return(prod)
}




# x = seq(-1,1, length = 1000)
# y = seq(0, length = 1000)
# for (i in 1:1000){
#   y[i] = coxlikeli(x[i])
# 
# }
# 
# plot(0,0,xlim = c(-1,1),ylim = c(-0.1,0),type = "n")
# lines(x,y)

# PSO(coxlikeli, optimType = "MAX", numVar = 1, numPopulation = 100,
#      maxIter = 1000, rangeVar = matrix(c(-1,1),2,1)
# , Vmax = 2, ci = 1.49445,
#      cg = 1.49445, w = 0.729)

  coxlikeli(-0.02667477)
  coxlikeli(-0.05283)


```

